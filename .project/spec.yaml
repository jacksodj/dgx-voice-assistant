specVersion: v2
specMinorVersion: 2
meta:
  name: dgx-voice-assistant
  image: project-dgx-voice-assistant
  description: Production-ready voice assistant powered by Qwen3-Omni-30B with vLLM inference
  labels:
    - llm
    - voice-assistant
    - qwen3
    - vllm
    - docker-compose
  createdOn: "2025-11-10T00:00:00Z"
  defaultBranch: main
layout:
- path: code/
  type: code
  storage: git
- path: backend/
  type: code
  storage: git
- path: data/
  type: data
  storage: gitignore
environment:
  base:
    registry: nvcr.io
    image: nvidia/cuda:12.2.0-runtime-ubuntu22.04
    build_timestamp: "20241110000000"
    name: CUDA Runtime
    supported_architectures:
      - amd64
      - arm64
    cuda_version: "12.2"
    description: NVIDIA CUDA 12.2 runtime for running docker-compose multi-container application
    entrypoint_script: ""
    labels:
    - cuda12.2
    - docker-compose
    - vllm
    apps: []
    programming_languages:
    - python3
    icon_url: ""
    image_version: 1.0.0
    os: linux
    os_distro: ubuntu
    os_distro_release: "22.04"
    schema_version: v2
    user_info:
      uid: ""
      gid: ""
      username: ""
    package_managers:
    - name: apt
      binary_path: /usr/bin/apt
      installed_packages:
      - curl
      - git
      - docker.io
      - docker-compose
    - name: pip
      binary_path: /usr/bin/pip
      installed_packages: []
    package_manager_environment:
      name: ""
      target: ""
  compose_file_path: docker-compose.complete.yml
execution:
  apps:
  - name: Voice Assistant Web UI
    type: custom
    class: webapp
    start_command: ""
    health_check_command: curl -f "http://localhost:3000/" || exit 1
    stop_command: ""
    user_msg: "Access the voice assistant web interface (start via Environment > Compose)"
    logfile_path: ""
    timeout_seconds: 60
    icon_url: ""
    webapp_options:
      autolaunch: false
      port: "3000"
      proxy:
        trim_prefix: false
      url: http://localhost:3000/
  - name: vLLM API Server
    type: custom
    class: webapp
    start_command: ""
    health_check_command: curl -f "http://localhost:8000/health" || exit 1
    stop_command: ""
    user_msg: "OpenAI-compatible API endpoint (start via Environment > Compose)"
    logfile_path: ""
    timeout_seconds: 60
    icon_url: ""
    webapp_options:
      autolaunch: false
      port: "8000"
      proxy:
        trim_prefix: false
      url: http://localhost:8000/docs
  - name: Backend API
    type: custom
    class: webapp
    start_command: ""
    health_check_command: curl -f "http://localhost:8080/health" || exit 1
    stop_command: ""
    user_msg: "Enhanced backend with search and storage (start via Environment > Compose)"
    logfile_path: ""
    timeout_seconds: 60
    icon_url: ""
    webapp_options:
      autolaunch: false
      port: "8080"
      proxy:
        trim_prefix: false
      url: http://localhost:8080/docs
  resources:
    gpu:
      requested: 1
    sharedMemoryMB: 2048
  secrets:
  - variable: HF_TOKEN
    description: "Hugging Face token for accessing gated models (optional)"
  - variable: BRAVE_API_KEY
    description: "Brave Search API key for internet search capabilities"
  mounts:
  - type: project
    target: /project/
    description: Project directory
    options: rw
