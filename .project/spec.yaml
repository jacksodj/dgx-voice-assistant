specVersion: v2
specMinorVersion: 1

meta:
  name: dgx-voice-assistant
  image: project-dgx-voice-assistant
  description: Production-ready voice assistant powered by Qwen3-Omni-30B with internet search and persistent storage
  labels:
    - llm
    - voice-assistant
    - qwen3
    - vllm
    - gpu-optimized
  createdOn: "2025-11-10T00:00:00Z"
  defaultBranch: main

layout:
  - path: backend/
    type: code
    storage: git
  - path: data/
    type: data
    storage: gitignore

environment:
  base:
    registry: docker.io
    image: vllm/vllm-openai:latest
    build_timestamp: ""
    name: vLLM OpenAI
    supported_architectures:
      - amd64
      - arm64
    cuda_version: "12.2"
    description: vLLM inference server with OpenAI API compatibility
    entrypoint_script: ""
    labels:
      - vllm
      - cuda12.2

  variables:
    HF_TOKEN:
      type: secret
      description: "Hugging Face token for accessing gated models (optional)"
      value: ""

    BRAVE_API_KEY:
      type: secret
      description: "Brave Search API key for internet search capabilities"
      value: ""

    VLLM_API_URL:
      type: string
      description: "Internal vLLM API endpoint"
      value: "http://vllm:8000"

    ENABLE_SEARCH:
      type: string
      description: "Enable internet search capabilities"
      value: "true"

    GPU_MEMORY_UTILIZATION:
      type: string
      description: "Fraction of GPU memory to use (0.0-1.0)"
      value: "0.85"

  mounts:
    - type: project
      target: /project/
      description: Project directory

  apps:
    - name: Web UI
      type: custom
      class: webapp
      start_command: docker-compose -f /project/docker-compose.complete.yml up webui
      health_check_command: "curl -f http://localhost:3000 || exit 1"
      stop_command: docker-compose -f /project/docker-compose.complete.yml stop webui
      icon_url: ""
      webapp_options:
        autolaunch: true
        port: "3000"
        proxy:
          trim_prefix: false
      user_msg: "Access the voice assistant web interface"

    - name: vLLM API
      type: custom
      class: webapp
      start_command: ""
      health_check_command: "curl -f http://localhost:8000/health || exit 1"
      stop_command: ""
      icon_url: ""
      webapp_options:
        autolaunch: false
        port: "8000"
        proxy:
          trim_prefix: false
      user_msg: "OpenAI-compatible API endpoint for the Qwen3-Omni model"

    - name: Backend API
      type: custom
      class: webapp
      start_command: ""
      health_check_command: "curl -f http://localhost:8080/health || exit 1"
      stop_command: ""
      icon_url: ""
      webapp_options:
        autolaunch: false
        port: "8080"
        proxy:
          trim_prefix: false
      user_msg: "Enhanced backend with search and storage capabilities"

execution:
  resources:
    gpu:
      required: true
      count: 1
      min_memory_gb: 16
    sharedMemoryMB: 2048
